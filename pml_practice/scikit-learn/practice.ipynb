{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classifiers Using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"No Free Lunch Theorem\" by David H. Wolpert\n",
    "\n",
    "No single classifier works best across all possible scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:\n",
    "\n",
    "- Selecting features and collecting labeled training examples.\n",
    "- Choosing a performance metric.\n",
    "- Choosing a classifier and optimization of algorithm.\n",
    "- Evaluating the performance of the model.\n",
    "- Tuning the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with scikit-learn - training a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Class labels: [0 1 2]\n"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.3,\n",
    "    random_state = 13,\n",
    "    stratify = y \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "\n",
    "Please notice that 'stratify' returns training and test subsets that have the same proportion of class labels as the input dataset.\n",
    "\n",
    "We can use NumPy's \"bincount\" function, which counts the number of occurrences of each value in an array, to verifu that this is indeed the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Labels counts in y: [50 50 50]\nLabels counts in y_train: [35 35 35]\nLabels counts in y_test: [35 35 35]\n"
    }
   ],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling using `StandardScaler`\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "1. StandardScaler object needs to be created.\n",
    "2. `fit` method shall be used on the created object in order to estimate the following parameters for each feature dimension from the training data:\n",
    "    - sample mean\n",
    "    - standard deviation\n",
    "3. `transform` method will then standardized the training data using those estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() # StandardScaler object\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training preceptron model\n",
    "\n",
    "Most alforithms in scikit-learn aleady support multiclass classification by default via the **one-vs.-rest (OvR)** method.\n",
    "\n",
    "`Perceptron` class will be created with the following parameters:\n",
    "    - eta0: learning rate\n",
    "    - random_state\n",
    "\n",
    "Please note that the learning rate requires some experimentation. If the learning rate is too large, the algorithm will overshoot the global cost minimum. If the learning rate is too small, the algorith will require more epochs until convergence, which can make the learning slow (especially for large datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=0.1,\n           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n           penalty=None, random_state=13, shuffle=True, tol=0.001,\n           validation_fraction=0.1, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(\n",
    "    eta0 = 0.1,\n",
    "    random_state = 13\n",
    "    )\n",
    "\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bit7ce9bcd6e9604f49a75ac00b61501ba1",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}